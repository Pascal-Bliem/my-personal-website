<%- include("partials/header") -%>

<!-- Special fonts for title -->
<link
  href="https://fonts.googleapis.com/css2?family=Bangers&display=swap"
  rel="stylesheet"
/>
<link
  href="https://fonts.googleapis.com/css2?family=Creepster&display=swap"
  rel="stylesheet"
/>

<div class="container mt-5 post-page">
  <div class="row">
    <div class="col-lg-8 col-md-10 col-sm-12 ml-auto mr-auto">
      <h1 class="toxblock-title">
        <span class="tox-span">Tox</span><span class="block-span">Block </span>
      </h1>
    </div>
  </div>
  <div class="row">
    <div class="col-lg-8 col-md-10 col-sm-12 ml-auto mr-auto post-title-body">
      <p class="">
        ToxBlock is a machine learning application for recognizing toxic
        language in text. It can potentially be employed for automatically
        screening text in articles, posts, and comments on social media, digital
        news, online forums etc. and blocking it or flagging it for further
        review by human intelligence. It can predict probabilities for
        classifying English text into six categories of verbal toxicity: toxic,
        severe toxic, obscene, threat, insult, and identity hate. 
        You can <strong><a href="#try">try it out</a></strong> below.
      </p>
      <p>
        The underlying ToxBlock application is a machine learning package
        written in Python, which is being served as a containerized REST API to
        which HTTP requests with potentially toxic text can be send over the internet.
        I previously published a couple of resources on the application:
        <ul>
          <li><a href="https://github.com/Pascal-Bliem/tox-block">The ToxBlock Github repository</a></li>
          <li><a href="https://github.com/Pascal-Bliem/tox-block-api">The ToxBlock API Github repository</a></li>
          <li><a href="https://pypi.org/project/tox-block/">ToxBlock on the Python package index (PyPI)</a></li>
          <li><a href="http://www.pascal-bliem.com/blog/tox%20block%20using%20ai%20to%20keep%20discussions%20clean">A blog post on the ToxBlock package</a></li>
          <li><a href="http://www.pascal-bliem.com/blog/the%20tox%20block%20api%20bring%20ml%20models%20into%20action%20by%20serving%20them%20on%20the%20web">A blog post on the ToxBlock API</a></li>
        </ul>
      </p>
      <h3>How it works</h3>
      <p>
        I got into deeper detail in the afore mentioned blog posts, but to sum it up shortly, 
        the workflow is as follows: The input data is send in JSON format
        via a HTTP POST request to the API app, which is hosted on a cloud instance at <a href="https://www.heroku.com/">Heroku</a>. 
        The app is deployed as a <a href="https://www.docker.com/">Docker</a> container, which contains a webserver running the 
        <a href="https://flask.palletsprojects.com/en/1.1.x/">Flask</a>
        micro web framework. Flask provides the routes to the API endpoints, through which the actual functionality of ToxBlock is served.</p> 
        <img class="toxblock-img" src="https://pb-data-blogposts.s3.eu-central-1.amazonaws.com/tox-block/workflow.png" alt="ToxBlock workflow">
      <p>Inside of the ToxBlock package is where the real action happens: the text input is encoded numerically and embedded using 
        <a href="https://nlp.stanford.edu/projects/glove/">pretrained</a> word vectors, and then fed into a recurrent neural network, 
        which has been trained on about 250000 <a href="https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data">text samples</a>. 
        The neural network predicts the probabilities of a given input text belonging to any of the six categories of verbal toxicity,
        and finally, the prediction is returned in JSON format via the HTTP response.<a id="try"></a>
      </p>
      
      <h3>Try it out</h3>
      <p>
        Type an input text into the text field below, post it to the ToxBlock
        API, and get back the toxicity classification. The API goes into
        hibernation after being inactive for a while, so it may take a few seconds
        to wake up again.
      </p>
      <form id="toxblock-form" action="" method="POST">
        <p
          >API status:
          <span id="toxblock-app-status" class="toxblock-app-status"
            >waking up</span
          ></p
        >
        <div class="toxblock-form-control">
          <label for="toxblock-input">Input Text</label>
          <textarea
            name="toxblock-input"
            class="toxblock-input"
            id="toxblock-input"
            cols="30"
            rows="5"
            placeholder="Type something nasty..."
          ></textarea>
          <small>ERROR MESSAGE</small>
        </div>
        <div class="toxblock-post-button-div">
          <button
            type="submit"
            class="btn btn-dark btn-sm toxblock-post-button"
          >
            POST
          </button>
        </div>
      </form>
      <div id="toxblock-chart" class="toxblock-chart">
        <div class="row">
          <div class="col-lg-2 col-sm-10">
            <p class="toxblock-bar-label">Toxic</p>
          </div>
          <div class="col-lg-10 col-sm-10">
            <div class="toxblock-bar" id="toxic-bar"></div>
            <p class="toxblock-bar-number" id="toxic-bar-number">0%</p>
          </div>
        </div>
        <div class="row">
          <div class="col-lg-2 col-sm-10">
            <p class="toxblock-bar-label">Severe toxic</p>
          </div>
          <div class="col-lg-10 col-sm-10">
            <div class="toxblock-bar" id="severe-toxic-bar"></div>
            <p class="toxblock-bar-number" id="severe-toxic-bar-number">0%</p>
          </div>
        </div>
        <div class="row">
          <div class="col-lg-2 col-sm-10">
            <p class="toxblock-bar-label">Obscene</p>
          </div>
          <div class="col-lg-10 col-sm-10">
            <div class="toxblock-bar" id="obscene-bar"></div>
            <p class="toxblock-bar-number" id="obscene-bar-number">0%</p>
          </div>
        </div>
        <div class="row">
          <div class="col-lg-2 col-sm-10">
            <p class="toxblock-bar-label">Insult</p>
          </div>
          <div class="col-lg-10 col-sm-10">
            <div class="toxblock-bar" id="insult-bar"></div>
            <p class="toxblock-bar-number" id="insult-bar-number">0%</p>
          </div>
        </div>
        <div class="row">
          <div class="col-lg-2 col-sm-10">
            <p class="toxblock-bar-label">Threat</p>
          </div>
          <div class="col-lg-10 col-sm-10">
            <div class="toxblock-bar" id="threat-bar"></div>
            <p class="toxblock-bar-number" id="threat-bar-number">0%</p>
          </div>
        </div>
        <div class="row">
          <div class="col-lg-2 col-sm-10">
            <p class="toxblock-bar-label">Identity hate</p>
          </div>
          <div class="col-lg-10 col-sm-10">
            <div class="toxblock-bar" id="identity-hate-bar"></div>
            <p class="toxblock-bar-number" id="identity-hate-bar-number">0%</p>
          </div>
        </div>
      </div>
    </div>
    <script src="../scripts/toxblockForm.js"></script>
  </div>
</div>

<!-- toxic, severe toxic, obscene, insult, threat, and identity hate -->

<%- include("partials/footer") -%>
